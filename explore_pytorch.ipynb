{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1T6uYwBdBfwI3iJMK-59xNYn_o2CjCnvm","authorship_tag":"ABX9TyMt5JMNjVUAz9vaCxN8ClZh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## **Pytorch Installation on Local Machine**"],"metadata":{"id":"Kb0fan9TT64g"}},{"cell_type":"markdown","source":["To install pytorch, we need to go to pytorch website. Scroll down until we found installation method for pytorch. If we are windows and linux user, we can use cuda support for our pytorch and download cuda to our local machine."],"metadata":{"id":"0jFE9pXhUTBI"}},{"cell_type":"markdown","source":["## **Tensor define value**"],"metadata":{"id":"TUUD4En7rkzU"}},{"cell_type":"code","source":["import torch\n","# Create tensor matrix with various dimensions\n","a = torch.empty(2) # matrix tensor 1,2 \n","b = torch.empty(2,1) # matrix tensor 2,1\n","\n","# Create tensor matrix with only 1 value with various dimensions\n","c = torch.ones(2,2, dtype=torch.float16) # this matrix contain only number 1 with 2,2 size and float16 data type\n","\n","# Create tensor matrix with our custom value\n","d = torch.tensor([2.5, 0.1]) # will return tensor matrix with 2.5 and 0.1 value with 1,2 dimensions\n","\n","# Create tensor with random value\n","e = torch.rand(5,3) # will return tensor with random value in matrix dimension 5,3\n","\n","# Reshape tensor size\n","c = torch.rand(2,2)\n","y = c.view(4) # tensor will return matrix with 1 dimension. The value 4 appear is because tensor c has 4 value (with matrix 2,2)\n","\n","# Check tensor size\n","c.size()"],"metadata":{"id":"l83t1rAdXe4z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Tensor calculation**"],"metadata":{"id":"-QLboWrWrn1i"}},{"cell_type":"code","source":["# Simple sum of 2 value in torch\n","z = a + b # will return new tensor matrix with sum value of matrix a + b and same dimension\n","\n","# add new value in other tensor\n","a.add_(b) # this mean value in tensor a add with value in tensor b, sign _ mean that this calculation inplace=True\n","\n","# tensor subtraction\n","z = torch.sub(x,y) # has same meaning with z = x-y\n","\n","# tensor multiplication\n","z = torch.mul(x,y) # has same meaning with z = x*y\n","\n","# tensor divide\n","z = torch.div(x,y) # has same meaning with z = x/y"],"metadata":{"id":"DSLnXeH6XmSV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **IMPORTANT TENSOR OPERATION WITH CUDA**"],"metadata":{"id":"9h10Q8jBrraV"}},{"cell_type":"markdown","source":["If we use tensor operation without cuda, there is no problem. But, if we use cuda for tensor operation, there is problem in update value on converting numpy array format into tensor format.\n","\n","One problem is, when we define numpy array like a = np.array([1,2,3,4]) and we turn this numpy array into tensor with b = torch.tensor(a) and if we change this tensor b value with mathematical operation like b.add_(b), tensor b will return value tensor(2,4,6,8), but array a will also update it value to array(2,4,6,8). We don't want this condition.\n"],"metadata":{"id":"3FerGIv6yJtT"}},{"cell_type":"code","source":["# Here code that we can use to tackle problem aforementioned above\n","if torch.cuda.is_available(): # check cuda available\n","  device = torch.device('cuda') # define cuda variable\n","  a = torch.ones(5, device=device) # define tensor a\n","  b = torch.ones(5) # define tensor b\n","  b = b.to(device) # turn tensor b from CPU to GPU\n","\n","  # operation in GPU\n","  z = a + b\n","\n","  # If we want to do operation with numpy data type, so we must turn from GPU to CPU. It because numpy only process data with CPU\n","  z = z.to('cpu') "],"metadata":{"id":"X29RUoU5rfdK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Autograd operation**"],"metadata":{"id":"xvgh5Sl-4R2z"}},{"cell_type":"code","source":["a = torch.tensor([1,2], dtype=torch.float64, requires_grad=True) # requires_grad is mandatory to perform autograd operation\n","b = torch.tensor([6,7], dtype=torch.float64, requires_grad=True)\n","z = a*b\n","\n","# backpropagation\n","z.backward(torch.tensor([1.,1.])) # tensor([1.,1.]) needed based on jacobian matrix formula\n","\n","# gradient result in specific variable that we want to operate\n","a.grad # this mean gradient operation happened with formula dz/da"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_BcJenvsFYxQ","executionInfo":{"status":"ok","timestamp":1680773010887,"user_tz":-420,"elapsed":744,"user":{"displayName":"Wahyu Dwi Nugraha","userId":"00900073614664055541"}},"outputId":"5ea69e11-732f-4b5c-d948-6f2ecf9317ab"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([6., 7.], dtype=torch.float64)"]},"metadata":{},"execution_count":81}]},{"cell_type":"markdown","source":["For further explanation about jacobian matrix, see this link:\n","https://drive.google.com/file/d/16Np-DIMIukqAMvzxqRzldVhWtyPW4XWW/view?usp=share_link\n"],"metadata":{"id":"cmWmD0CX2jNN"}},{"cell_type":"code","source":["# Way to remove grade function in tensor\n","a = torch.tensor([1.,2.,3.], requires_grad=True)\n","print(a)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EGRYSNACR5kg","executionInfo":{"status":"ok","timestamp":1680773573008,"user_tz":-420,"elapsed":582,"user":{"displayName":"Wahyu Dwi Nugraha","userId":"00900073614664055541"}},"outputId":"b43dbf4b-3dc0-442f-a660-ba7d42c51584"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1., 2., 3.], requires_grad=True)\n"]}]},{"cell_type":"code","source":["# Way 1\n","a.requires_grad_(False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9frs4d_D6WKh","executionInfo":{"status":"ok","timestamp":1680773590090,"user_tz":-420,"elapsed":453,"user":{"displayName":"Wahyu Dwi Nugraha","userId":"00900073614664055541"}},"outputId":"77635265-0de1-4274-ae67-c131cacb7578"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1., 2., 3.])"]},"metadata":{},"execution_count":83}]},{"cell_type":"code","source":["# Way 2\n","a.detach()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DsT4vLuk6aS7","executionInfo":{"status":"ok","timestamp":1680773622400,"user_tz":-420,"elapsed":617,"user":{"displayName":"Wahyu Dwi Nugraha","userId":"00900073614664055541"}},"outputId":"93aabeff-019e-4757-c7fd-45e64e820187"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1., 2., 3.])"]},"metadata":{},"execution_count":84}]},{"cell_type":"code","source":["# Way 3\n","with torch.no_grad():\n","  print(a)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dNnfO6GT6iCX","executionInfo":{"status":"ok","timestamp":1680773681836,"user_tz":-420,"elapsed":409,"user":{"displayName":"Wahyu Dwi Nugraha","userId":"00900073614664055541"}},"outputId":"7451c0f6-da84-4a91-c078-7ae2e1070a8c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1., 2., 3.])\n"]}]},{"cell_type":"markdown","source":["## **Training example**"],"metadata":{"id":"Q-J59_k_6z6N"}},{"cell_type":"code","source":["import torch"],"metadata":{"id":"ztJ7XXmOcdZt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a = torch.tensor([1.,2.,3.,4.], requires_grad=True)\n","\n","for i in range(3):\n","  output = a**2\n","  output.backward(torch.ones(4))\n","  print(a.grad)\n","\n","  a.grad.zero_() # need to reset backward to zero for next operation"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TFtClgxv_GeD","executionInfo":{"status":"ok","timestamp":1681090942793,"user_tz":-420,"elapsed":6,"user":{"displayName":"Wahyu Dwi Nugraha","userId":"00900073614664055541"}},"outputId":"60bfce3d-b32f-4dab-d1c0-bf96cc502f19"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([2., 4., 6., 8.])\n","tensor([2., 4., 6., 8.])\n","tensor([2., 4., 6., 8.])\n"]}]},{"cell_type":"markdown","source":["## **Create example of update weight with gradient descent**"],"metadata":{"id":"zNtCYT58TmQx"}},{"cell_type":"markdown","source":["with this example, we want to show how weight can be updated on function. Final weight we compare with actual function"],"metadata":{"id":"RvIJk1gxT3hW"}},{"cell_type":"code","source":["# Define actual function\n","import numpy as np\n","x = np.array([1,2,3,4], dtype=np.float32)\n","y = np.array([2,4,6,8], dtype=np.float32)"],"metadata":{"id":"QyEIqDjGT0Cv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["from data above, we can see that our baseline (actual) function is y = 2*x"],"metadata":{"id":"g4a0fu01UfNr"}},{"cell_type":"code","source":["# Define raw function with weight\n","def forward(x):\n","  y_prediction = w*x\n","  return y_prediction\n","\n","def loss_function(y_prediction,y):\n","  loss = ((y_prediction-y)**2).mean()\n","  return loss\n","\n","def gradient_descent(w,x,y):\n","  gradient = 2*w*(x**2) - 2*x*y\n","  return gradient\n","\n","w = 0.0\n","learning_rate = 0.01\n","\n","for i in range(400):\n","  y_prediction = forward(x)\n","\n","  loss = loss_function(y_prediction, y)\n","\n","  gradient = gradient_descent(w, x, y)\n","\n","  print(f'w = {w} and loss = {loss}')\n","\n","  w = w - (learning_rate*gradient)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eDghjdAmUedN","executionInfo":{"status":"ok","timestamp":1681050313477,"user_tz":-420,"elapsed":882,"user":{"displayName":"Wahyu Dwi Nugraha","userId":"00900073614664055541"}},"outputId":"cb24a4bc-162f-4fb2-b1f6-5af7b49b9aa4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["w = 0.0 and loss = 30.0\n","w = [0.04       0.16       0.35999998 0.64      ] and loss = 17.796001434326172\n","w = [0.0792 0.3072 0.6552 1.0752] and loss = 11.278056144714355\n","w = [0.117616   0.44262403 0.897264   1.371136  ] and loss = 7.629202365875244\n","w = [0.15526368 0.56721413 1.0957565  1.5723724 ] and loss = 5.474826335906982\n","w = [0.1921584 0.681837  1.2585204 1.7092133] and loss = 4.129886150360107\n","w = [0.22831523 0.78729004 1.3919867  1.802265  ] and loss = 3.243558883666992\n","w = [0.26374894 0.88430685 1.5014291  1.8655403 ] and loss = 2.6300199031829834\n","w = [0.29847395 0.9735623  1.5911719  1.9085674 ] and loss = 2.186877727508545\n","w = [0.33250448 1.0556773  1.664761   1.9378258 ] and loss = 1.8552099466323853\n","w = [0.36585438 1.1312231  1.725104   1.9577216 ] and loss = 1.599558711051941\n","w = [0.39853728 1.2007252  1.7745852  1.9712507 ] and loss = 1.3976436853408813\n","w = [0.43056652 1.2646672  1.8151599  1.9804504 ] and loss = 1.2348966598510742\n","w = [0.4619552 1.3234937 1.8484311 1.9867063] and loss = 1.1014525890350342\n","w = [0.49271607 1.3776143  1.8757135  1.9909602 ] and loss = 0.9904232025146484\n","w = [0.5228618 1.4274051 1.898085  1.993853 ] and loss = 0.8968704342842102\n","w = [0.5524045 1.4732127 1.9164296 1.99582  ] and loss = 0.8171719312667847\n","w = [0.5813564 1.5153557 1.9314723 1.9971576] and loss = 0.7486159205436707\n","w = [0.6097293 1.5541272 1.9438072 1.9980671] and loss = 0.6891353130340576\n","w = [0.6375347 1.589797  1.9539219 1.9986856] and loss = 0.6371285915374756\n","w = [0.66478395 1.6226133  1.962216   1.9991062 ] and loss = 0.5913365483283997\n","w = [0.69148827 1.6528043  1.9690171  1.9993922 ] and loss = 0.5507569313049316\n","w = [0.7176585 1.6805799 1.974594  1.9995867] and loss = 0.514582097530365\n","w = [0.7433053 1.7061335 1.9791671 1.9997189] and loss = 0.4821547567844391\n","w = [0.76843923 1.7296427  1.9829171  1.9998089 ] and loss = 0.45293524861335754\n","w = [0.79307044 1.7512714  1.985992   1.9998701 ] and loss = 0.42647725343704224\n","w = [0.817209  1.7711697 1.9885134 1.9999117] and loss = 0.4024088680744171\n","w = [0.84086484 1.789476   1.9905809  1.9999399 ] and loss = 0.3804185092449188\n","w = [0.8640475 1.8063179 1.9922763 1.9999591] and loss = 0.3602439761161804\n","w = [0.88676655 1.8218125  1.9936665  1.9999722 ] and loss = 0.3416631817817688\n","w = [0.9090312 1.8360676 1.9948065 1.9999812] and loss = 0.3244878053665161\n","w = [0.93085057 1.8491821  1.9957414  1.9999872 ] and loss = 0.3085569739341736\n","w = [0.95223355 1.8612475  1.9965079  1.9999913 ] and loss = 0.2937333285808563\n","w = [0.9731889 1.8723477 1.9971365 1.999994 ] and loss = 0.27989882230758667\n","w = [0.9937251 1.8825599 1.9976519 1.999996 ] and loss = 0.26695188879966736\n","w = [1.0138506 1.8919551 1.9980747 1.9999973] and loss = 0.25480473041534424\n","w = [1.0335735 1.9005988 1.9984212 1.9999981] and loss = 0.24338124692440033\n","w = [1.052902  1.9085509 1.9987054 1.9999987] and loss = 0.2326153814792633\n","w = [1.071844  1.9158667 1.9989384 1.9999992] and loss = 0.22244934737682343\n","w = [1.0904071 1.9225974 1.9991295 1.9999994] and loss = 0.21283265948295593\n","w = [1.108599  1.9287896 1.9992862 1.9999996] and loss = 0.2037210315465927\n","w = [1.1264269 1.9344864 1.9994147 1.9999998] and loss = 0.19507527351379395\n","w = [1.1438984 1.9397274 1.9995201 1.9999999] and loss = 0.1868607997894287\n","w = [1.1610204 1.9445492 1.9996065 1.9999999] and loss = 0.17904682457447052\n","w = [1.1777999 1.9489852 1.9996773 1.9999999] and loss = 0.17160597443580627\n","w = [1.1942439 1.9530663 1.9997354 1.9999999] and loss = 0.16451364755630493\n","w = [1.210359  1.9568211 1.999783  1.9999999] and loss = 0.1577477604150772\n","w = [1.2261518 1.9602754 1.9998221 1.9999999] and loss = 0.15128837525844574\n","w = [1.2416288 1.9634534 1.9998541 1.9999999] and loss = 0.14511743187904358\n","w = [1.2567962 1.9663771 1.9998803 1.9999999] and loss = 0.13921847939491272\n","w = [1.2716603 1.969067  1.9999019 1.9999999] and loss = 0.13357652723789215\n","w = [1.2862271 1.9715416 1.9999195 1.9999999] and loss = 0.12817782163619995\n","w = [1.3005025 1.9738183 1.9999341 1.9999999] and loss = 0.12300966680049896\n","w = [1.3144925 1.9759128 1.999946  1.9999999] and loss = 0.11806034296751022\n","w = [1.3282026 1.9778398 1.9999558 1.9999999] and loss = 0.1133190169930458\n","w = [1.3416386 1.9796126 1.9999638 1.9999999] and loss = 0.10877559334039688\n","w = [1.3548058 1.9812436 1.9999703 1.9999999] and loss = 0.10442068427801132\n","w = [1.3677098 1.9827441 1.9999757 1.9999999] and loss = 0.10024549812078476\n","w = [1.3803556 1.9841245 1.9999801 1.9999999] and loss = 0.09624182432889938\n","w = [1.3927485 1.9853946 1.9999837 1.9999999] and loss = 0.09240192174911499\n","w = [1.4048935 1.9865631 1.9999866 1.9999999] and loss = 0.088718481361866\n","w = [1.4167956 1.987638  1.999989  1.9999999] and loss = 0.08518465608358383\n","w = [1.4284596 1.988627  1.999991  1.9999999] and loss = 0.08179394155740738\n","w = [1.4398905 1.9895368 1.9999926 1.9999999] and loss = 0.07854013890028\n","w = [1.4510927 1.9903738 1.9999939 1.9999999] and loss = 0.07541746646165848\n","w = [1.4620708 1.991144  1.999995  1.9999999] and loss = 0.0724203810095787\n","w = [1.4728295 1.9918524 1.999996  1.9999999] and loss = 0.06954357773065567\n","w = [1.4833729 1.9925042 1.9999967 1.9999999] and loss = 0.06678206473588943\n","w = [1.4937055 1.9931039 1.9999973 1.9999999] and loss = 0.06413108110427856\n","w = [1.5038314 1.9936556 1.9999977 1.9999999] and loss = 0.06158607453107834\n","w = [1.5137547 1.9941632 1.9999982 1.9999999] and loss = 0.05914268270134926\n","w = [1.5234796 1.9946301 1.9999986 1.9999999] and loss = 0.056796763092279434\n","w = [1.53301   1.9950597 1.9999988 1.9999999] and loss = 0.054544322192668915\n","w = [1.5423498 1.9954549 1.999999  1.9999999] and loss = 0.05238157883286476\n","w = [1.5515028 1.9958185 1.9999992 1.9999999] and loss = 0.05030491575598717\n","w = [1.5604727 1.996153  1.9999993 1.9999999] and loss = 0.04831085726618767\n","w = [1.5692632 1.9964608 1.9999994 1.9999999] and loss = 0.04639606922864914\n","w = [1.577878  1.9967439 1.9999995 1.9999999] and loss = 0.044557347893714905\n","w = [1.5863204 1.9970044 1.9999996 1.9999999] and loss = 0.04279167577624321\n","w = [1.594594  1.997244  1.9999998 1.9999999] and loss = 0.04109610244631767\n","w = [1.6027021 1.9974645 1.9999998 1.9999999] and loss = 0.03946782648563385\n","w = [1.6106482 1.9976674 1.9999998 1.9999999] and loss = 0.03790415823459625\n","w = [1.6184351 1.997854  1.9999998 1.9999999] and loss = 0.036402538418769836\n","w = [1.6260664 1.9980257 1.9999998 1.9999999] and loss = 0.03496047109365463\n","w = [1.6335452 1.9981836 1.9999998 1.9999999] and loss = 0.03357558697462082\n","w = [1.6408743 1.9983289 1.9999998 1.9999999] and loss = 0.03224561735987663\n","w = [1.6480567 1.9984626 1.9999998 1.9999999] and loss = 0.030968377366662025\n","w = [1.6550956 1.9985856 1.9999998 1.9999999] and loss = 0.029741765931248665\n","w = [1.6619936 1.9986987 1.9999998 1.9999999] and loss = 0.02856377139687538\n","w = [1.6687537 1.9988028 1.9999998 1.9999999] and loss = 0.027432454749941826\n","w = [1.6753787 1.9988985 1.9999998 1.9999999] and loss = 0.026345962658524513\n","w = [1.681871  1.9989866 1.9999998 1.9999999] and loss = 0.02530253306031227\n","w = [1.6882336 1.9990677 1.9999998 1.9999999] and loss = 0.024300439283251762\n","w = [1.694469  1.9991423 1.9999998 1.9999999] and loss = 0.023338038474321365\n","w = [1.7005796 1.999211  1.9999998 1.9999999] and loss = 0.02241376042366028\n","w = [1.706568  1.9992741 1.9999998 1.9999999] and loss = 0.021526111289858818\n","w = [1.7124367 1.9993322 1.9999998 1.9999999] and loss = 0.020673612132668495\n","w = [1.7181879 1.9993856 1.9999998 1.9999999] and loss = 0.019854888319969177\n","w = [1.7238241 1.9994347 1.9999998 1.9999999] and loss = 0.01906859688460827\n","w = [1.7293477 1.9994799 1.9999998 1.9999999] and loss = 0.018313435837626457\n","w = [1.7347608 1.9995215 1.9999998 1.9999999] and loss = 0.017588192597031593\n","w = [1.7400656 1.9995598 1.9999998 1.9999999] and loss = 0.01689166948199272\n","w = [1.7452643 1.9995949 1.9999998 1.9999999] and loss = 0.01622273400425911\n","w = [1.750359  1.9996274 1.9999998 1.9999999] and loss = 0.01558028906583786\n","w = [1.7553519 1.9996572 1.9999998 1.9999999] and loss = 0.01496329065412283\n","w = [1.7602448 1.9996846 1.9999998 1.9999999] and loss = 0.014370732940733433\n","w = [1.7650399 1.9997098 1.9999998 1.9999999] and loss = 0.013801643624901772\n","w = [1.7697392 1.9997331 1.9999998 1.9999999] and loss = 0.013255085796117783\n","w = [1.7743443 1.9997544 1.9999998 1.9999999] and loss = 0.012730181217193604\n","w = [1.7788575 1.9997741 1.9999998 1.9999999] and loss = 0.012226056307554245\n","w = [1.7832804 1.9997922 1.9999998 1.9999999] and loss = 0.01174189243465662\n","w = [1.7876148 1.9998088 1.9999998 1.9999999] and loss = 0.011276902630925179\n","w = [1.7918625 1.999824  1.9999998 1.9999999] and loss = 0.010830337181687355\n","w = [1.7960253 1.9998381 1.9999998 1.9999999] and loss = 0.010401448234915733\n","w = [1.8001047 1.9998511 1.9999998 1.9999999] and loss = 0.009989551268517971\n","w = [1.8041027 1.999863  1.9999998 1.9999999] and loss = 0.009593960829079151\n","w = [1.8080206 1.999874  1.9999998 1.9999999] and loss = 0.009214038960635662\n","w = [1.8118602 1.9998841 1.9999998 1.9999999] and loss = 0.008849158883094788\n","w = [1.815623  1.9998934 1.9999998 1.9999999] and loss = 0.008498726412653923\n","w = [1.8193105 1.999902  1.9999998 1.9999999] and loss = 0.008162179030478\n","w = [1.8229244 1.9999099 1.9999998 1.9999999] and loss = 0.007838952355086803\n","w = [1.8264658 1.999917  1.9999998 1.9999999] and loss = 0.007528532762080431\n","w = [1.8299365 1.9999237 1.9999998 1.9999999] and loss = 0.007230403833091259\n","w = [1.8333378 1.9999298 1.9999998 1.9999999] and loss = 0.006944078486412764\n","w = [1.836671  1.9999354 1.9999998 1.9999999] and loss = 0.006669095251709223\n","w = [1.8399376 1.9999405 1.9999998 1.9999999] and loss = 0.006404999177902937\n","w = [1.8431388 1.9999453 1.9999998 1.9999999] and loss = 0.006151360925287008\n","w = [1.846276  1.9999497 1.9999998 1.9999999] and loss = 0.005907766055315733\n","w = [1.8493506 1.9999537 1.9999998 1.9999999] and loss = 0.005673815030604601\n","w = [1.8523636 1.9999574 1.9999998 1.9999999] and loss = 0.005449129734188318\n","w = [1.8553163 1.9999609 1.9999998 1.9999999] and loss = 0.00523334601894021\n","w = [1.85821   1.999964  1.9999998 1.9999999] and loss = 0.005026104860007763\n","w = [1.8610457 1.9999669 1.9999998 1.9999999] and loss = 0.0048270742408931255\n","w = [1.8638248 1.9999695 1.9999998 1.9999999] and loss = 0.004635919351130724\n","w = [1.8665483 1.9999719 1.9999998 1.9999999] and loss = 0.004452339839190245\n","w = [1.8692173 1.9999741 1.9999998 1.9999999] and loss = 0.004276030696928501\n","w = [1.871833  1.9999762 1.9999998 1.9999999] and loss = 0.004106697626411915\n","w = [1.8743963 1.9999781 1.9999998 1.9999999] and loss = 0.003944071475416422\n","w = [1.8769084 1.9999799 1.9999998 1.9999999] and loss = 0.003787884721532464\n","w = [1.8793702 1.9999815 1.9999998 1.9999999] and loss = 0.003637886606156826\n","w = [1.8817828 1.999983  1.9999998 1.9999999] and loss = 0.003493828698992729\n","w = [1.8841472 1.9999843 1.9999998 1.9999999] and loss = 0.0033554700203239918\n","w = [1.8864642 1.9999856 1.9999998 1.9999999] and loss = 0.0032225926406681538\n","w = [1.8887349 1.9999868 1.9999998 1.9999999] and loss = 0.0030949788633733988\n","w = [1.8909602 1.9999878 1.9999998 1.9999999] and loss = 0.0029724189080297947\n","w = [1.893141  1.9999888 1.9999998 1.9999999] and loss = 0.002854709979146719\n","w = [1.8952782 1.9999897 1.9999998 1.9999999] and loss = 0.0027416630182415247\n","w = [1.8973726 1.9999906 1.9999998 1.9999999] and loss = 0.002633095718920231\n","w = [1.8994251 1.9999913 1.9999998 1.9999999] and loss = 0.0025288250762969255\n","w = [1.9014367 1.999992  1.9999998 1.9999999] and loss = 0.0024286815896630287\n","w = [1.9034079 1.9999926 1.9999998 1.9999999] and loss = 0.0023325069341808558\n","w = [1.9053397 1.9999932 1.9999998 1.9999999] and loss = 0.0022401423193514347\n","w = [1.9072329 1.9999938 1.9999998 1.9999999] and loss = 0.0021514345426112413\n","w = [1.9090883 1.9999943 1.9999998 1.9999999] and loss = 0.0020662364549934864\n","w = [1.9109064 1.9999948 1.9999998 1.9999999] and loss = 0.0019844158086925745\n","w = [1.9126883 1.9999952 1.9999998 1.9999999] and loss = 0.0019058352336287498\n","w = [1.9144344 1.9999956 1.9999998 1.9999999] and loss = 0.001830366556532681\n","w = [1.9161458 1.999996  1.9999998 1.9999999] and loss = 0.0017578816041350365\n","w = [1.9178228 1.9999963 1.9999998 1.9999999] and loss = 0.001688271528109908\n","w = [1.9194664 1.9999965 1.9999998 1.9999999] and loss = 0.0016214161878451705\n","w = [1.921077  1.9999968 1.9999998 1.9999999] and loss = 0.0015572094125673175\n","w = [1.9226555 1.999997  1.9999998 1.9999999] and loss = 0.001495544333010912\n","w = [1.9242023 1.9999973 1.9999998 1.9999999] and loss = 0.001436321996152401\n","w = [1.9257183 1.9999975 1.9999998 1.9999999] and loss = 0.001379442517645657\n","w = [1.9272039 1.9999977 1.9999998 1.9999999] and loss = 0.0013248182367533445\n","w = [1.9286598 1.999998  1.9999998 1.9999999] and loss = 0.0012723561376333237\n","w = [1.9300866 1.9999981 1.9999998 1.9999999] and loss = 0.001221970422193408\n","w = [1.9314849 1.9999982 1.9999998 1.9999999] and loss = 0.0011735784355551004\n","w = [1.9328552 1.9999983 1.9999998 1.9999999] and loss = 0.0011271043913438916\n","w = [1.9341981 1.9999985 1.9999998 1.9999999] and loss = 0.0010824711062014103\n","w = [1.9355142 1.9999986 1.9999998 1.9999999] and loss = 0.001039604190737009\n","w = [1.9368039 1.9999987 1.9999998 1.9999999] and loss = 0.0009984355419874191\n","w = [1.9380679 1.9999988 1.9999998 1.9999999] and loss = 0.0009588958346284926\n","w = [1.9393065 1.9999989 1.9999998 1.9999999] and loss = 0.0009209252893924713\n","w = [1.9405204 1.999999  1.9999998 1.9999999] and loss = 0.000884455512277782\n","w = [1.94171   1.9999992 1.9999998 1.9999999] and loss = 0.0008494311477988958\n","w = [1.9428757 1.9999993 1.9999998 1.9999999] and loss = 0.0008157952106557786\n","w = [1.9440182 1.9999993 1.9999998 1.9999999] and loss = 0.0007834892021492124\n","w = [1.9451379 1.9999993 1.9999998 1.9999999] and loss = 0.0007524636457674205\n","w = [1.9462351 1.9999993 1.9999998 1.9999999] and loss = 0.0007226672023534775\n","w = [1.9473103 1.9999993 1.9999998 1.9999999] and loss = 0.0006940503953956068\n","w = [1.9483641 1.9999993 1.9999998 1.9999999] and loss = 0.0006665655528195202\n","w = [1.9493968 1.9999993 1.9999998 1.9999999] and loss = 0.0006401697173714638\n","w = [1.9504089 1.9999993 1.9999998 1.9999999] and loss = 0.0006148184183984995\n","w = [1.9514008 1.9999993 1.9999998 1.9999999] and loss = 0.0005904716090299189\n","w = [1.9523728 1.9999993 1.9999998 1.9999999] and loss = 0.0005670877872034907\n","w = [1.9533254 1.9999993 1.9999998 1.9999999] and loss = 0.0005446297582238913\n","w = [1.9542589 1.9999993 1.9999998 1.9999999] and loss = 0.0005230616079643369\n","w = [1.9551737 1.9999993 1.9999998 1.9999999] and loss = 0.0005023485864512622\n","w = [1.9560703 1.9999993 1.9999998 1.9999999] and loss = 0.0004824545467272401\n","w = [1.9569489 1.9999993 1.9999998 1.9999999] and loss = 0.00046334980288520455\n","w = [1.9578099 1.9999993 1.9999998 1.9999999] and loss = 0.00044500059448182583\n","w = [1.9586537 1.9999993 1.9999998 1.9999999] and loss = 0.0004273793601896614\n","w = [1.9594806 1.9999993 1.9999998 1.9999999] and loss = 0.0004104545805603266\n","w = [1.960291  1.9999993 1.9999998 1.9999999] and loss = 0.000394200615119189\n","w = [1.9610852 1.9999993 1.9999998 1.9999999] and loss = 0.00037859039730392396\n","w = [1.9618635 1.9999993 1.9999998 1.9999999] and loss = 0.0003635978209786117\n","w = [1.9626262 1.9999993 1.9999998 1.9999999] and loss = 0.00034919989411719143\n","w = [1.9633737 1.9999993 1.9999998 1.9999999] and loss = 0.0003353721695020795\n","w = [1.9641062 1.9999993 1.9999998 1.9999999] and loss = 0.00032209118944592774\n","w = [1.9648241 1.9999993 1.9999998 1.9999999] and loss = 0.00030933631933294237\n","w = [1.9655277 1.9999993 1.9999998 1.9999999] and loss = 0.0002970856730826199\n","w = [1.966217  1.9999993 1.9999998 1.9999999] and loss = 0.0002853220794349909\n","w = [1.9668927 1.9999993 1.9999998 1.9999999] and loss = 0.0002740230120252818\n","w = [1.9675548 1.9999993 1.9999998 1.9999999] and loss = 0.00026317263836972415\n","w = [1.9682037 1.9999993 1.9999998 1.9999999] and loss = 0.00025275174994021654\n","w = [1.9688396 1.9999993 1.9999998 1.9999999] and loss = 0.00024274192401207983\n","w = [1.9694629 1.9999993 1.9999998 1.9999999] and loss = 0.00023312905977945775\n","w = [1.9700736 1.9999993 1.9999998 1.9999999] and loss = 0.00022389764490071684\n","w = [1.9706721 1.9999993 1.9999998 1.9999999] and loss = 0.00021503098832909018\n","w = [1.9712586 1.9999993 1.9999998 1.9999999] and loss = 0.0002065164444502443\n","w = [1.9718335 1.9999993 1.9999998 1.9999999] and loss = 0.00019833838450722396\n","w = [1.9723969 1.9999993 1.9999998 1.9999999] and loss = 0.00019048346439376473\n","w = [1.9729489 1.9999993 1.9999998 1.9999999] and loss = 0.00018294037727173418\n","w = [1.9734899 1.9999993 1.9999998 1.9999999] and loss = 0.0001756966084940359\n","w = [1.9740201 1.9999993 1.9999998 1.9999999] and loss = 0.00016873849381227046\n","w = [1.9745398 1.9999993 1.9999998 1.9999999] and loss = 0.0001620559924049303\n","w = [1.975049  1.9999993 1.9999998 1.9999999] and loss = 0.0001556378701934591\n","w = [1.975548  1.9999993 1.9999998 1.9999999] and loss = 0.00014947472664061934\n","w = [1.976037  1.9999993 1.9999998 1.9999999] and loss = 0.00014355604071170092\n","w = [1.9765162 1.9999993 1.9999998 1.9999999] and loss = 0.00013787166972178966\n","w = [1.9769859 1.9999993 1.9999998 1.9999999] and loss = 0.0001324118347838521\n","w = [1.9774462 1.9999993 1.9999998 1.9999999] and loss = 0.00012716848868876696\n","w = [1.9778973 1.9999993 1.9999998 1.9999999] and loss = 0.0001221324928337708\n","w = [1.9783393 1.9999993 1.9999998 1.9999999] and loss = 0.0001172963238786906\n","w = [1.9787725 1.9999993 1.9999998 1.9999999] and loss = 0.00011265146895311773\n","w = [1.979197  1.9999993 1.9999998 1.9999999] and loss = 0.00010819093586178496\n","w = [1.9796131 1.9999993 1.9999998 1.9999999] and loss = 0.00010390677198302001\n","w = [1.9800208 1.9999993 1.9999998 1.9999999] and loss = 9.979249443858862e-05\n","w = [1.9804204 1.9999993 1.9999998 1.9999999] and loss = 9.584065992385149e-05\n","w = [1.980812  1.9999993 1.9999998 1.9999999] and loss = 9.204528032569215e-05\n","w = [1.9811957 1.9999993 1.9999998 1.9999999] and loss = 8.840053487801924e-05\n","w = [1.9815718 1.9999993 1.9999998 1.9999999] and loss = 8.489970059599727e-05\n","w = [1.9819404 1.9999993 1.9999998 1.9999999] and loss = 8.153739327099174e-05\n","w = [1.9823016 1.9999993 1.9999998 1.9999999] and loss = 7.83084033173509e-05\n","w = [1.9826555 1.9999993 1.9999998 1.9999999] and loss = 7.520770304836333e-05\n","w = [1.9830024 1.9999993 1.9999998 1.9999999] and loss = 7.222939893836156e-05\n","w = [1.9833424 1.9999993 1.9999998 1.9999999] and loss = 6.93688343744725e-05\n","w = [1.9836756 1.9999993 1.9999998 1.9999999] and loss = 6.66215128148906e-05\n","w = [1.9840021 1.9999993 1.9999998 1.9999999] and loss = 6.398309778887779e-05\n","w = [1.9843221 1.9999993 1.9999998 1.9999999] and loss = 6.144936196506023e-05\n","w = [1.9846356 1.9999993 1.9999998 1.9999999] and loss = 5.901626354898326e-05\n","w = [1.9849429 1.9999993 1.9999998 1.9999999] and loss = 5.667896766681224e-05\n","w = [1.985244  1.9999993 1.9999998 1.9999999] and loss = 5.4434622143162414e-05\n","w = [1.9855392 1.9999993 1.9999998 1.9999999] and loss = 5.227869769441895e-05\n","w = [1.9858284 1.9999993 1.9999998 1.9999999] and loss = 5.0208564061904326e-05\n","w = [1.9861119 1.9999993 1.9999998 1.9999999] and loss = 4.8219972086371854e-05\n","w = [1.9863896 1.9999993 1.9999998 1.9999999] and loss = 4.6310495235957205e-05\n","w = [1.9866618 1.9999993 1.9999998 1.9999999] and loss = 4.4476950279204175e-05\n","w = [1.9869286 1.9999993 1.9999998 1.9999999] and loss = 4.271549187251367e-05\n","w = [1.98719   1.9999993 1.9999998 1.9999999] and loss = 4.10239736083895e-05\n","w = [1.9874462 1.9999993 1.9999998 1.9999999] and loss = 3.9399543311446905e-05\n","w = [1.9876972 1.9999993 1.9999998 1.9999999] and loss = 3.783945430768654e-05\n","w = [1.9879433 1.9999993 1.9999998 1.9999999] and loss = 3.6341054510558024e-05\n","w = [1.9881845 1.9999993 1.9999998 1.9999999] and loss = 3.4901793696917593e-05\n","w = [1.9884207 1.9999993 1.9999998 1.9999999] and loss = 3.3519903809065e-05\n","w = [1.9886523 1.9999993 1.9999998 1.9999999] and loss = 3.219229984097183e-05\n","w = [1.9888793 1.9999993 1.9999998 1.9999999] and loss = 3.091736289206892e-05\n","w = [1.9891018 1.9999993 1.9999998 1.9999999] and loss = 2.9692868338315748e-05\n","w = [1.9893197 1.9999993 1.9999998 1.9999999] and loss = 2.8517297323560342e-05\n","w = [1.9895333 1.9999993 1.9999998 1.9999999] and loss = 2.7387925001676194e-05\n","w = [1.9897426 1.9999993 1.9999998 1.9999999] and loss = 2.6303376216674224e-05\n","w = [1.9899478 1.9999993 1.9999998 1.9999999] and loss = 2.5261702830903232e-05\n","w = [1.9901488 1.9999993 1.9999998 1.9999999] and loss = 2.4261620637844317e-05\n","w = [1.9903458 1.9999993 1.9999998 1.9999999] and loss = 2.3300721295527183e-05\n","w = [1.990539  1.9999993 1.9999998 1.9999999] and loss = 2.2377844288712367e-05\n","w = [1.9907281 1.9999993 1.9999998 1.9999999] and loss = 2.1491847292054445e-05\n","w = [1.9909136 1.9999993 1.9999998 1.9999999] and loss = 2.064053296635393e-05\n","w = [1.9910953 1.9999993 1.9999998 1.9999999] and loss = 1.982340108952485e-05\n","w = [1.9912734 1.9999993 1.9999998 1.9999999] and loss = 1.903837255667895e-05\n","w = [1.9914479 1.9999993 1.9999998 1.9999999] and loss = 1.8284494217368774e-05\n","w = [1.991619  1.9999993 1.9999998 1.9999999] and loss = 1.756032906996552e-05\n","w = [1.9917866 1.9999993 1.9999998 1.9999999] and loss = 1.686498762865085e-05\n","w = [1.9919509 1.9999993 1.9999998 1.9999999] and loss = 1.619712566025555e-05\n","w = [1.9921118 1.9999993 1.9999998 1.9999999] and loss = 1.555591734359041e-05\n","w = [1.9922695 1.9999993 1.9999998 1.9999999] and loss = 1.4940096662030555e-05\n","w = [1.9924241 1.9999993 1.9999998 1.9999999] and loss = 1.4348451259138528e-05\n","w = [1.9925756 1.9999993 1.9999998 1.9999999] and loss = 1.3780260815110523e-05\n","w = [1.9927242 1.9999993 1.9999998 1.9999999] and loss = 1.3234389371064026e-05\n","w = [1.9928697 1.9999993 1.9999998 1.9999999] and loss = 1.2710171176877338e-05\n","w = [1.9930123 1.9999993 1.9999998 1.9999999] and loss = 1.2206956853333395e-05\n","w = [1.993152  1.9999993 1.9999998 1.9999999] and loss = 1.1723700481525157e-05\n","w = [1.993289  1.9999993 1.9999998 1.9999999] and loss = 1.1259401617280673e-05\n","w = [1.9934232 1.9999993 1.9999998 1.9999999] and loss = 1.0813498192874249e-05\n","w = [1.9935547 1.9999993 1.9999998 1.9999999] and loss = 1.0385437235527206e-05\n","w = [1.9936836 1.9999993 1.9999998 1.9999999] and loss = 9.974301974580158e-06\n","w = [1.9938099 1.9999993 1.9999998 1.9999999] and loss = 9.579216566635296e-06\n","w = [1.9939338 1.9999993 1.9999998 1.9999999] and loss = 9.199706255458295e-06\n","w = [1.9940552 1.9999993 1.9999998 1.9999999] and loss = 8.835305379761849e-06\n","w = [1.994174  1.9999993 1.9999998 1.9999999] and loss = 8.485559192195069e-06\n","w = [1.9942905 1.9999993 1.9999998 1.9999999] and loss = 8.149680979840923e-06\n","w = [1.9944047 1.9999993 1.9999998 1.9999999] and loss = 7.826920409570448e-06\n","w = [1.9945166 1.9999993 1.9999998 1.9999999] and loss = 7.516889127145987e-06\n","w = [1.9946263 1.9999993 1.9999998 1.9999999] and loss = 7.2192078732769005e-06\n","w = [1.9947338 1.9999993 1.9999998 1.9999999] and loss = 6.933189069968648e-06\n","w = [1.9948392 1.9999993 1.9999998 1.9999999] and loss = 6.6584871092345566e-06\n","w = [1.9949424 1.9999993 1.9999998 1.9999999] and loss = 6.394762749550864e-06\n","w = [1.9950436 1.9999993 1.9999998 1.9999999] and loss = 6.1413884395733476e-06\n","w = [1.9951428 1.9999993 1.9999998 1.9999999] and loss = 5.898055860598106e-06\n","w = [1.99524   1.9999993 1.9999998 1.9999999] and loss = 5.664464879373554e-06\n","w = [1.9953352 1.9999993 1.9999998 1.9999999] and loss = 5.440040695248172e-06\n","w = [1.9954286 1.9999993 1.9999998 1.9999999] and loss = 5.2245113693061285e-06\n","w = [1.99552   1.9999993 1.9999998 1.9999999] and loss = 5.017610419599805e-06\n","w = [1.9956096 1.9999993 1.9999998 1.9999999] and loss = 4.81881397718098e-06\n","w = [1.9956975 1.9999993 1.9999998 1.9999999] and loss = 4.627881025953684e-06\n","w = [1.9957836 1.9999993 1.9999998 1.9999999] and loss = 4.444576916284859e-06\n","w = [1.9958678 1.9999993 1.9999998 1.9999999] and loss = 4.268669727025554e-06\n","w = [1.9959505 1.9999993 1.9999998 1.9999999] and loss = 4.0996937968884595e-06\n","w = [1.9960314 1.9999993 1.9999998 1.9999999] and loss = 3.937439942092169e-06\n","w = [1.9961108 1.9999993 1.9999998 1.9999999] and loss = 3.781475925279665e-06\n","w = [1.9961886 1.9999993 1.9999998 1.9999999] and loss = 3.631615754784434e-06\n","w = [1.9962648 1.9999993 1.9999998 1.9999999] and loss = 3.4879017221101094e-06\n","w = [1.9963396 1.9999993 1.9999998 1.9999999] and loss = 3.3497067306598183e-06\n","w = [1.9964128 1.9999993 1.9999998 1.9999999] and loss = 3.2170839858736144e-06\n","w = [1.9964845 1.9999993 1.9999998 1.9999999] and loss = 3.089654001087183e-06\n","w = [1.9965549 1.9999993 1.9999998 1.9999999] and loss = 2.967262616948574e-06\n","w = [1.9966238 1.9999993 1.9999998 1.9999999] and loss = 2.8497590847109677e-06\n","w = [1.9966912 1.9999993 1.9999998 1.9999999] and loss = 2.7369953841116512e-06\n","w = [1.9967574 1.9999993 1.9999998 1.9999999] and loss = 2.6286336378689157e-06\n","w = [1.9968222 1.9999993 1.9999998 1.9999999] and loss = 2.524543560866732e-06\n","w = [1.9968858 1.9999993 1.9999998 1.9999999] and loss = 2.424597596473177e-06\n","w = [1.996948  1.9999993 1.9999998 1.9999999] and loss = 2.328670916540432e-06\n","w = [1.997009  1.9999993 1.9999998 1.9999999] and loss = 2.2364627056958852e-06\n","w = [1.9970689 1.9999993 1.9999998 1.9999999] and loss = 2.1478638245753245e-06\n","w = [1.9971275 1.9999993 1.9999998 1.9999999] and loss = 2.0627674075512914e-06\n","w = [1.997185  1.9999993 1.9999998 1.9999999] and loss = 1.981068407985731e-06\n","w = [1.9972413 1.9999993 1.9999998 1.9999999] and loss = 1.9026642803510185e-06\n","w = [1.9972965 1.9999993 1.9999998 1.9999999] and loss = 1.8272930901730433e-06\n","w = [1.9973506 1.9999993 1.9999998 1.9999999] and loss = 1.754865934344707e-06\n","w = [1.9974036 1.9999993 1.9999998 1.9999999] and loss = 1.685295728748315e-06\n","w = [1.9974556 1.9999993 1.9999998 1.9999999] and loss = 1.6184974356292514e-06\n","w = [1.9975065 1.9999993 1.9999998 1.9999999] and loss = 1.5543871541012777e-06\n","w = [1.9975563 1.9999993 1.9999998 1.9999999] and loss = 1.4928830296412343e-06\n","w = [1.9976052 1.9999993 1.9999998 1.9999999] and loss = 1.4337620086735114e-06\n","w = [1.9976531 1.9999993 1.9999998 1.9999999] and loss = 1.376954287479748e-06\n","w = [1.9977001 1.9999993 1.9999998 1.9999999] and loss = 1.3223911992099602e-06\n","w = [1.9977461 1.9999993 1.9999998 1.9999999] and loss = 1.2700058960035676e-06\n","w = [1.9977912 1.9999993 1.9999998 1.9999999] and loss = 1.219731984747341e-06\n","w = [1.9978354 1.9999993 1.9999998 1.9999999] and loss = 1.1713764251908287e-06\n","w = [1.9978787 1.9999993 1.9999998 1.9999999] and loss = 1.1250101579207694e-06\n","w = [1.9979211 1.9999993 1.9999998 1.9999999] and loss = 1.0804474186443258e-06\n","w = [1.9979627 1.9999993 1.9999998 1.9999999] and loss = 1.0376350019214442e-06\n","w = [1.9980035 1.9999993 1.9999998 1.9999999] and loss = 9.96520839180448e-07\n","w = [1.9980434 1.9999993 1.9999998 1.9999999] and loss = 9.570539987180382e-07\n","w = [1.9980825 1.9999993 1.9999998 1.9999999] and loss = 9.191845720124547e-07\n","w = [1.9981209 1.9999993 1.9999998 1.9999999] and loss = 8.827512942843896e-07\n","w = [1.9981585 1.9999993 1.9999998 1.9999999] and loss = 8.478228892272455e-07\n","w = [1.9981953 1.9999993 1.9999998 1.9999999] and loss = 8.142448564285587e-07\n","w = [1.9982314 1.9999993 1.9999998 1.9999999] and loss = 7.819775760253833e-07\n","w = [1.9982668 1.9999993 1.9999998 1.9999999] and loss = 7.509823944928939e-07\n","w = [1.9983015 1.9999993 1.9999998 1.9999999] and loss = 7.212212267404539e-07\n","w = [1.9983355 1.9999993 1.9999998 1.9999999] and loss = 6.926569540155469e-07\n","w = [1.9983687 1.9999993 1.9999998 1.9999999] and loss = 6.652530259998457e-07\n","w = [1.9984014 1.9999993 1.9999998 1.9999999] and loss = 6.388785322997137e-07\n","w = [1.9984334 1.9999993 1.9999998 1.9999999] and loss = 6.135976491350448e-07\n","w = [1.9984647 1.9999993 1.9999998 1.9999999] and loss = 5.892845820199e-07\n","w = [1.9984955 1.9999993 1.9999998 1.9999999] and loss = 5.659113071487809e-07\n","w = [1.9985255 1.9999993 1.9999998 1.9999999] and loss = 5.435381353890989e-07\n","w = [1.998555  1.9999993 1.9999998 1.9999999] and loss = 5.220467755862046e-07\n","w = [1.9985838 1.9999993 1.9999998 1.9999999] and loss = 5.014109092371655e-07\n","w = [1.9986122 1.9999993 1.9999998 1.9999999] and loss = 4.815219654119574e-07\n","w = [1.99864   1.9999993 1.9999998 1.9999999] and loss = 4.624407665687613e-07\n","w = [1.9986671 1.9999993 1.9999998 1.9999999] and loss = 4.4414244371182576e-07\n","w = [1.9986938 1.9999993 1.9999998 1.9999999] and loss = 4.265248492174578e-07\n","w = [1.9987199 1.9999993 1.9999998 1.9999999] and loss = 4.0964516756503144e-07\n","w = [1.9987456 1.9999993 1.9999998 1.9999999] and loss = 3.9340531543530233e-07\n","w = [1.9987706 1.9999993 1.9999998 1.9999999] and loss = 3.7786020357089e-07\n","w = [1.9987952 1.9999993 1.9999998 1.9999999] and loss = 3.629156424267421e-07\n","w = [1.9988192 1.9999993 1.9999998 1.9999999] and loss = 3.485540673864307e-07\n","w = [1.9988428 1.9999993 1.9999998 1.9999999] and loss = 3.347582548940409e-07\n","w = [1.998866  1.9999993 1.9999998 1.9999999] and loss = 3.21511322454171e-07\n","w = [1.9988886 1.9999993 1.9999998 1.9999999] and loss = 3.0879672863193264e-07\n","w = [1.9989108 1.9999993 1.9999998 1.9999999] and loss = 2.965982730529504e-07\n","w = [1.9989326 1.9999993 1.9999998 1.9999999] and loss = 2.8483646019594744e-07\n","w = [1.9989539 1.9999993 1.9999998 1.9999999] and loss = 2.735619659688382e-07\n","w = [1.9989748 1.9999993 1.9999998 1.9999999] and loss = 2.627594994919491e-07\n","w = [1.9989953 1.9999993 1.9999998 1.9999999] and loss = 2.5235425482605933e-07\n","w = [1.9990155 1.9999993 1.9999998 1.9999999] and loss = 2.423352327696193e-07\n","w = [1.9990351 1.9999993 1.9999998 1.9999999] and loss = 2.3274914440207795e-07\n","w = [1.9990544 1.9999993 1.9999998 1.9999999] and loss = 2.2352554651661194e-07\n","w = [1.9990734 1.9999993 1.9999998 1.9999999] and loss = 2.1465407940013392e-07\n","w = [1.9990919 1.9999993 1.9999998 1.9999999] and loss = 2.0617871143713273e-07\n","w = [1.99911   1.9999993 1.9999998 1.9999999] and loss = 1.9803316320121667e-07\n","w = [1.9991277 1.9999993 1.9999998 1.9999999] and loss = 1.9020771446776052e-07\n","w = [1.9991452 1.9999993 1.9999998 1.9999999] and loss = 1.8269284396410512e-07\n","w = [1.9991622 1.9999993 1.9999998 1.9999999] and loss = 1.7547921515870257e-07\n","w = [1.999179  1.9999993 1.9999998 1.9999999] and loss = 1.6850873407747713e-07\n","w = [1.9991955 1.9999993 1.9999998 1.9999999] and loss = 1.6182335116354807e-07\n","w = [1.9992115 1.9999993 1.9999998 1.9999999] and loss = 1.5541424147613725e-07\n","w = [1.9992273 1.9999993 1.9999998 1.9999999] and loss = 1.4927276481557783e-07\n","w = [1.9992428 1.9999993 1.9999998 1.9999999] and loss = 1.433453462595935e-07\n","w = [1.9992579 1.9999993 1.9999998 1.9999999] and loss = 1.3767066775471903e-07\n","w = [1.9992728 1.9999993 1.9999998 1.9999999] and loss = 1.321972717960307e-07\n","w = [1.9992874 1.9999993 1.9999998 1.9999999] and loss = 1.269622913468993e-07\n","w = [1.9993017 1.9999993 1.9999998 1.9999999] and loss = 1.2191630105462536e-07\n","w = [1.9993156 1.9999993 1.9999998 1.9999999] and loss = 1.1709497016454407e-07\n","w = [1.9993293 1.9999993 1.9999998 1.9999999] and loss = 1.1245084863276134e-07\n","w = [1.9993428 1.9999993 1.9999998 1.9999999] and loss = 1.0797901950354571e-07\n","w = [1.9993559 1.9999993 1.9999998 1.9999999] and loss = 1.0371306302658923e-07\n","w = [1.9993688 1.9999993 1.9999998 1.9999999] and loss = 9.960831448552199e-08\n"]}]},{"cell_type":"markdown","source":["from 400 iteration above, we can see that weight for every value on y = w*x is 1.99 (very close to 2). This prove that our backpropagation algorithm work well."],"metadata":{"id":"TcolAnq3aIwW"}},{"cell_type":"markdown","source":["## **Update weight with torch gradient descent (autograd)**"],"metadata":{"id":"vRIntPaby2FN"}},{"cell_type":"code","source":["import torch\n","import numpy as np"],"metadata":{"id":"KkO3iH0Lae-W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = torch.tensor([1,2,3,4], dtype=torch.float64)\n","y = torch.tensor([2,4,6,8], dtype=torch.float64)"],"metadata":{"id":"NLmjJ5QubCkn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define raw function with weight\n","def forward(x):\n","  y_prediction = w*x\n","  return y_prediction\n","\n","def loss_function(y_prediction,y):\n","  loss = ((y_prediction-y)**2).mean()\n","  return loss\n","\n","w = torch.tensor(0.0, dtype=torch.float64, requires_grad=True)\n","learning_rate = 0.01\n","\n","for i in range(40):\n","  y_prediction = forward(x)\n","\n","  loss = loss_function(y_prediction, y)\n","\n","  loss.backward()\n","\n","  print(f'w = {w} and loss = {loss}')\n","\n","  with torch.no_grad():  # why should use this?\n","    w -= learning_rate*(w.grad) # why w = w - (learning_rate*(w.grad)) not working?\n","\n","  w.grad.zero_()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kja62L9mzim3","executionInfo":{"status":"ok","timestamp":1681717647125,"user_tz":-420,"elapsed":319,"user":{"displayName":"Wahyu Dwi Nugraha","userId":"00900073614664055541"}},"outputId":"22a08b69-2a8c-4541-9718-fab7ef0dcfbe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["w = 0.0 and loss = 30.0\n","w = 0.3 and loss = 21.674999999999997\n","w = 0.5549999999999999 and loss = 15.6601875\n","w = 0.7717499999999999 and loss = 11.31448546875\n","w = 0.9559875 and loss = 8.174715751171876\n","w = 1.112589375 and loss = 5.90623213022168\n","w = 1.24570096875 and loss = 4.2672527140851635\n","w = 1.3588458234375 and loss = 3.08309008592653\n","w = 1.455018949921875 and loss = 2.2275325870819183\n","w = 1.5367661074335939 and loss = 1.609392294166685\n","w = 1.6062511913185549 and loss = 1.1627859325354297\n","w = 1.6653135126207717 and loss = 0.840112836256848\n","w = 1.715516485727656 and loss = 0.6069815241955727\n","w = 1.7581890128685076 and loss = 0.43854415123130097\n","w = 1.7944606609382314 and loss = 0.3168481492646149\n","w = 1.8252915617974967 and loss = 0.2289227878436842\n","w = 1.8514978275278722 and loss = 0.16539671421706198\n","w = 1.8737731533986914 and loss = 0.11949912602182704\n","w = 1.8927071803888877 and loss = 0.08633811855077002\n","w = 1.9088011033305545 and loss = 0.06237929065293141\n","w = 1.9224809378309713 and loss = 0.045069037496743035\n","w = 1.9341087971563256 and loss = 0.03256237959139684\n","w = 1.9439924775828767 and loss = 0.023526319254784246\n","w = 1.9523936059454452 and loss = 0.016997765661581662\n","w = 1.9595345650536284 and loss = 0.012280885690492713\n","w = 1.9656043802955843 and loss = 0.008872939911380941\n","w = 1.9707637232512467 and loss = 0.0064106990859727105\n","w = 1.9751491647635597 and loss = 0.004631730089615245\n","w = 1.9788767900490258 and loss = 0.0033464249897470046\n","w = 1.9820452715416719 and loss = 0.002417792055092241\n","w = 1.9847384808104211 and loss = 0.001746854759804122\n","w = 1.987027708688858 and loss = 0.0012621025639584968\n","w = 1.9889735523855292 and loss = 0.0009118691024600127\n","w = 1.9906275195277 and loss = 0.0006588254265273509\n","w = 1.9920333915985449 and loss = 0.0004760013706660163\n","w = 1.9932283828587631 and loss = 0.0003439109903062013\n","w = 1.9942441254299488 and loss = 0.00024847569049622146\n","w = 1.9951075066154564 and loss = 0.0001795236863835172\n","w = 1.9958413806231379 and loss = 0.0001297058634120986\n","w = 1.9964651735296672 and loss = 9.371248631524124e-05\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"zDQEUxakLeDQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Training neural network with pytorch tools**"],"metadata":{"id":"2asD9f-UQHLw"}},{"cell_type":"code","source":["import torch.nn as nn"],"metadata":{"id":"u2IJ1BNrTruc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define input and output variable with both values\n","x = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n","y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n","\n","x_test = torch.tensor([5], dtype=torch.float32)"],"metadata":{"id":"IEnzx-roKMnw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_samples, n_features = x.shape\n","\n","input_size = n_features\n","output_size = n_features\n","\n","model = nn.Linear(input_size, output_size)\n","\n","learning_rate = 0.01\n","iterations = 100\n","\n","loss = nn.MSELoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","for i in range(iterations):\n","  y_prediction = model(x) # for prediction\n","\n","  l = loss(y, y_prediction) # for calculating loss\n","\n","  l.backward() # for backpropagation\n","\n","  optimizer.step() # for update weights\n","\n","  optimizer.zero_grad() # reset gradient\n","\n","  [w,b] = model.parameters()\n","\n","  print(f'w = {w[0][0]} and loss = {l}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S9hos7fTTQX9","executionInfo":{"status":"ok","timestamp":1681099615705,"user_tz":-420,"elapsed":5,"user":{"displayName":"Wahyu Dwi Nugraha","userId":"00900073614664055541"}},"outputId":"93549e6f-6f6d-4c6e-ab07-418f862ee444"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["w = 0.5010972619056702 and loss = 22.154808044433594\n","w = 0.7159448862075806 and loss = 15.39315128326416\n","w = 0.8950178623199463 and loss = 10.701257705688477\n","w = 1.0442904233932495 and loss = 7.445530414581299\n","w = 1.1687391996383667 and loss = 5.186330795288086\n","w = 1.2725095748901367 and loss = 3.6185998916625977\n","w = 1.3590545654296875 and loss = 2.5306644439697266\n","w = 1.4312506914138794 and loss = 1.775651216506958\n","w = 1.4914939403533936 and loss = 1.2516456842422485\n","w = 1.5417802333831787 and loss = 0.8879327774047852\n","w = 1.58377206325531 and loss = 0.6354436278343201\n","w = 1.6188544034957886 and loss = 0.4601314067840576\n","w = 1.6481808423995972 and loss = 0.33837103843688965\n","w = 1.6727123260498047 and loss = 0.2537701725959778\n","w = 1.6932493448257446 and loss = 0.19495369493961334\n","w = 1.710458755493164 and loss = 0.15402951836585999\n","w = 1.72489595413208 and loss = 0.1255209892988205\n","w = 1.7370235919952393 and loss = 0.10562790930271149\n","w = 1.7472270727157593 and loss = 0.09171377122402191\n","w = 1.7558274269104004 and loss = 0.08194898813962936\n","w = 1.7630921602249146 and loss = 0.07506397366523743\n","w = 1.7692439556121826 and loss = 0.07017761468887329\n","w = 1.7744684219360352 and loss = 0.06667902320623398\n","w = 1.77892005443573 and loss = 0.0641438364982605\n","w = 1.7827277183532715 and loss = 0.06227784603834152\n","w = 1.7859985828399658 and loss = 0.060876838862895966\n","w = 1.788822054862976 and loss = 0.05979913845658302\n","w = 1.791272521018982 and loss = 0.058946363627910614\n","w = 1.7934120893478394 and loss = 0.05825028568506241\n","w = 1.7952922582626343 and loss = 0.05766349658370018\n","w = 1.7969560623168945 and loss = 0.05715325474739075\n","w = 1.798439383506775 and loss = 0.05669676885008812\n","w = 1.799772024154663 and loss = 0.05627816170454025\n","w = 1.8009788990020752 and loss = 0.05588637664914131\n","w = 1.802080750465393 and loss = 0.05551384389400482\n","w = 1.803094744682312 and loss = 0.055155325680971146\n","w = 1.8040354251861572 and loss = 0.05480718985199928\n","w = 1.8049145936965942 and loss = 0.054466620087623596\n","w = 1.8057422637939453 and loss = 0.05413215234875679\n","w = 1.806526780128479 and loss = 0.05380227416753769\n","w = 1.8072750568389893 and loss = 0.053476303815841675\n","w = 1.8079928159713745 and loss = 0.0531536266207695\n","w = 1.8086849451065063 and loss = 0.052833788096904755\n","w = 1.8093554973602295 and loss = 0.05251651629805565\n","w = 1.8100076913833618 and loss = 0.052201464772224426\n","w = 1.810644268989563 and loss = 0.05188867077231407\n","w = 1.811267614364624 and loss = 0.05157800391316414\n","w = 1.8118797540664673 and loss = 0.05126930773258209\n","w = 1.8124821186065674 and loss = 0.05096249654889107\n","w = 1.813076138496399 and loss = 0.05065767467021942\n","w = 1.8136630058288574 and loss = 0.05035466328263283\n","w = 1.8142435550689697 and loss = 0.05005355179309845\n","w = 1.8148185014724731 and loss = 0.04975421354174614\n","w = 1.8153886795043945 and loss = 0.04945668578147888\n","w = 1.8159544467926025 and loss = 0.04916098341345787\n","w = 1.816516399383545 and loss = 0.04886703938245773\n","w = 1.8170747756958008 and loss = 0.04857484996318817\n","w = 1.8176300525665283 and loss = 0.04828445613384247\n","w = 1.818182349205017 and loss = 0.04799576476216316\n","w = 1.8187319040298462 and loss = 0.04770882427692413\n","w = 1.8192789554595947 and loss = 0.04742348939180374\n","w = 1.8198236227035522 and loss = 0.047139957547187805\n","w = 1.8203661441802979 and loss = 0.04685812070965767\n","w = 1.8209065198898315 and loss = 0.046578001230955124\n","w = 1.8214448690414429 and loss = 0.04629955440759659\n","w = 1.8219811916351318 and loss = 0.046022672206163406\n","w = 1.822515606880188 and loss = 0.04574757069349289\n","w = 1.8230482339859009 and loss = 0.04547405242919922\n","w = 1.823578953742981 and loss = 0.0452021099627018\n","w = 1.8241080045700073 and loss = 0.04493191838264465\n","w = 1.8246352672576904 and loss = 0.04466328024864197\n","w = 1.8251608610153198 and loss = 0.04439619556069374\n","w = 1.8256847858428955 and loss = 0.044130779802799225\n","w = 1.8262070417404175 and loss = 0.04386693984270096\n","w = 1.8267277479171753 and loss = 0.04360467568039894\n","w = 1.8272467851638794 and loss = 0.043343912810087204\n","w = 1.8277642726898193 and loss = 0.04308484122157097\n","w = 1.8282800912857056 and loss = 0.04282723367214203\n","w = 1.8287943601608276 and loss = 0.042571116238832474\n","w = 1.8293070793151855 and loss = 0.042316656559705734\n","w = 1.8298182487487793 and loss = 0.042063597589731216\n","w = 1.8303278684616089 and loss = 0.04181214049458504\n","w = 1.8308359384536743 and loss = 0.04156215488910675\n","w = 1.8313424587249756 and loss = 0.04131366312503815\n","w = 1.8318474292755127 and loss = 0.04106663912534714\n","w = 1.8323508501052856 and loss = 0.04082109034061432\n","w = 1.832852840423584 and loss = 0.04057704284787178\n","w = 1.8333532810211182 and loss = 0.040334418416023254\n","w = 1.8338521718978882 and loss = 0.04009326547384262\n","w = 1.8343496322631836 and loss = 0.03985358774662018\n","w = 1.8348456621170044 and loss = 0.03961531072854996\n","w = 1.835340142250061 and loss = 0.039378486573696136\n","w = 1.8358330726623535 and loss = 0.03914298489689827\n","w = 1.8363245725631714 and loss = 0.03890896588563919\n","w = 1.8368146419525146 and loss = 0.03867633640766144\n","w = 1.8373032808303833 and loss = 0.03844509646296501\n","w = 1.8377903699874878 and loss = 0.03821522369980812\n","w = 1.8382760286331177 and loss = 0.03798677772283554\n","w = 1.838760256767273 and loss = 0.0377596840262413\n","w = 1.839242935180664 and loss = 0.03753387928009033\n"]}]},{"cell_type":"markdown","source":["## **Dataset and dataloader**"],"metadata":{"id":"ZyuDijrgaHbn"}},{"cell_type":"code","source":["import torch\n","import torchvision\n","from torch.utils.data import Dataset, Dataloader\n","import numpy as np\n","import math\n","\n","class WindDataset(Dataset):\n","  def __init__(self):\n","    xy = np.loadtxt('./data/wind/wind.csv', delimiter=',', dtype=np.float32, skiprows=1)\n","    self.x = torch.from_numpyr(xy[:, 1:])\n","    self.y = torch.from_numpy(xy[:, [0]]) # n_samples, 1\n","    self.n_samples = xy.shape[0]\n","\n","  def __getitem__(self, index):\n","    return self.x[index], self.y[index]\n","\n","  def __len__(self):\n","    return self.n_samples\n","\n","dataset = WindDataset()\n","first_data = dataset[0]"],"metadata":{"id":"jLpdFLapsoPO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Convert value type to tensor**"],"metadata":{"id":"H2cH4yQ1y7V8"}},{"cell_type":"code","source":["import torch\n","class tes:\n","  def __init__(self, angka_awal):\n","    self.angka_awal = angka_awal\n","\n","  def perkalian_1(self, koefisien_1):\n","    hasil_perkalian = self.angka_awal * koefisien_1\n","\n","    return hasil_perkalian\n","\n","  def to_tensor(self, fungsi):\n","    hasil_tensor = torch.tensor(fungsi)\n","\n","    return hasil_tensor"],"metadata":{"id":"LvplwfOIxO63"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test = tes(20)\n","test.perkalian_1(4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y3XKUuy0xrag","executionInfo":{"status":"ok","timestamp":1681462116907,"user_tz":-420,"elapsed":1087,"user":{"displayName":"Wahyu Dwi Nugraha","userId":"00900073614664055541"}},"outputId":"c4cd20a9-d4d9-42e7-909e-1d6ea512a2f4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["80"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["test.to_tensor(test.perkalian_1(4))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ito84MMpx493","executionInfo":{"status":"ok","timestamp":1681462117580,"user_tz":-420,"elapsed":3,"user":{"displayName":"Wahyu Dwi Nugraha","userId":"00900073614664055541"}},"outputId":"652a98ad-f1e4-4ebc-c8dc-387ae04c3aa4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(80)"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["## **Softmax activation function and cross entropy**"],"metadata":{"id":"WEnrch-tHwoJ"}},{"cell_type":"code","source":["import torch\n","import numpy as np"],"metadata":{"id":"UpFlcrlPH3Ul"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define softmax function without tensor\n","def softmax(x):\n","  return np.exp(x) / np.sum(np.exp(x), axis=0)\n","\n","x = np.array([2.0, 1.0, 0.1])\n","output = softmax(x)\n","\n","print(f'x values : {str(x)}')\n","print(f'softmax values : {str(output)}')"],"metadata":{"id":"FgRG2lX8ynSn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681700085907,"user_tz":-420,"elapsed":3,"user":{"displayName":"Wahyu Dwi Nugraha","userId":"00900073614664055541"}},"outputId":"7786edae-ae99-4aa8-c970-0d5c6c745fd6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x values : [2.  1.  0.1]\n","softmax values : [0.65900114 0.24243297 0.09856589]\n"]}]},{"cell_type":"code","source":["# define cross entropy function\n","def cross_entropy(actual, predicted):\n","  loss = -np.sum(actual * np.log(predicted))\n","  return loss\n","\n","y_actual = np.array([1,0,0])\n","\n","y_prediction_good = np.array([0.7, 0.2, 0.1])\n","y_prediction_bad = np.array([0.1, 0.3, 0.6])\n","\n","loss_1 = cross_entropy(y_actual, y_prediction_good)\n","loss_2 = cross_entropy (y_actual, y_prediction_bad)\n","\n","print(f'loss good : {loss_1}')\n","print(f'loss bad : {loss_2}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YnpsxdJHIbd2","executionInfo":{"status":"ok","timestamp":1681700808516,"user_tz":-420,"elapsed":412,"user":{"displayName":"Wahyu Dwi Nugraha","userId":"00900073614664055541"}},"outputId":"d80d2e7e-c34f-460e-cdc7-8bd11b8094e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["loss good : 0.35667494393873245\n","loss bad : 2.3025850929940455\n"]}]},{"cell_type":"markdown","source":["**Neural network with sigmoid**"],"metadata":{"id":"9G3x2lnrOyAa"}},{"cell_type":"code","source":["# Binary classification\n","class NeuralNet1(nn.Module):\n","  def __init__(self, input_size, hidden_size):\n","    super(NeuralNet1, self).__init__()\n","    self.linear1 = nn.Linear(input_size, hidden_size)\n","    self.relu = nn.ReLU()\n","    self.Linear2 = nn.Linear(hidden_size, 1)\n","\n","  def forward(self, x):\n","    out = self.Linear1(x)\n","    out = self.relu(out)\n","    out = self.Linear2(out)\n","    y_prediction = torch.sigmoid(out)\n","    return y_prediction\n","\n","model = NeuralNet1(input_size=28*28, hidden_size=5)\n","criterion = nn.BCELoss()  # binary cross entropy loss"],"metadata":{"id":"kqmnUzu1LcGt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Full example of feed forward neural network**"],"metadata":{"id":"WvNluO2BCPj2"}},{"cell_type":"code","source":["# define libraries\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","\n","# device config\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# hyperparameters\n","input_size = 784\n","hidden_size = 100\n","num_classes = 10\n","num_epochs = 2\n","batch_size = 100\n","learning_rate = 0.001\n","\n","# loading MNIST data\n","train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n","test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuflle=False)\n","\n","examples = iter(train_loader)\n","samples, labels = examples.next()\n","print(samples.shape, labels.shape)\n","\n","# Show image data\n","for i in range(6):\n","  plt.subplot(2, 3, i+1)\n","  plt.imshow(samples[i][0], cmap='gray')\n","\n","# Neural network\n","class NeuralNet(nn.Module):\n","  def __init__(self, input_size, hidden_size, num_classes):\n","    super(NeuralNet, self).__init__()\n","    self.l1 = nn.Linear(input_size, hidden_size)\n","    self.relu = nn.ReLU()\n","    self.l2 = nn.Linear(hidden_size, num_classes)\n","\n","  def forward(self, x):\n","    out = self.l1(x)\n","    out = self.relu(out)\n","    out = self.l2(out)\n","    return out\n","\n","# modeling\n","model = NeuralNet(input_size, hidden_size, num_classes)\n","\n","# loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# training loop\n","n_total_step = len(train_loader)\n","for epoch in range(num_epochs):\n","  for i, (images, labels) in enumerate(train_loader):\n","    # 100, 1, 28, 28\n","    # 100, 784\n","    images = images.reshape(-1, 28*28).to(device)\n","    labels = labels.to(device)\n","\n","    # forward\n","    output = model(images)\n","    loss = criterion(output, labels)\n","\n","    # backward\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    if (i+1 % 100 == 0):\n","      print(f'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_step}, loss = {loss.item():.4f}')\n","\n","\n","# test\n","with torch.no_grad(): # because we don't want to calculate gradient again\n","  n_correct = 0\n","  n_samples = 0\n","  for images, labels in test_loader:\n","    images = images.reshape(-1, 28*28).to(device)\n","    labels = labels.to(device)\n","    outputs = model(images)\n","\n","    # value, index\n","    _, predictions = torch.max(outputs, 1)\n","    n_samples += labels.shape[0]\n","    n_correct = (predictions == labels).sum().item()\n","\n","  acc = 100.0 * n_correct / n_samples\n","\n","  print(f'accuracy = {acc}')"],"metadata":{"id":"Qi7ehwciQkKB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Convolution neural network**"],"metadata":{"id":"h451GQQpNRbs"}},{"cell_type":"code","source":["class ConvNet(nn.Module):\n","  def __init__(self):\n","    super(ConvNet, self).__init__()\n","    self.conv1 = nn.Conv2d(3, 6, 5)  #3 is color channel, 6 is output channel, 5 is kernel size (5x5)\n","    self.pool = nn.MaxPool2d(2,2) #2 is kernel size, 2 is stride\n","    self.conv2 = nn.Conv2d(6, 16, 5)\n","    self.fc1 = nn.Linear(16*5*5, 120)\n","    self.fc2 = nn.Linear(120, 84)\n","    self.fc3 = nn.Linear(84, 10)  #10 is number of total class\n","\n","  def forward(self, x):\n","    x = self.pool(F.relu(self.conv1(x)))\n","    x = self.pool(F.relu(self.conv2(x)))\n","    x = x.view(-1, 16*5*5)  #for tensor flatten\n","    x = F.relu(self.fc1(x))\n","    x = F.relu(self.fc2(x))\n","    x = self.fc3(x)\n","    return x"],"metadata":{"id":"NQftzGRaNWhr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Transfer learning**"],"metadata":{"id":"srJHXFAYczW1"}},{"cell_type":"code","source":["from torchvision import models\n","model = models.resnet18(pretrained=True)\n","number_features = model.fc.in_features\n","\n","model.fc = nn.Linear(number_features, 2)\n","model.to(torch.device('cuda'))\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001)\n","\n","# scheduler\n","step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n","\n","model = train_model(model, criterion, optimizer, scheduler, num_epochs=20)"],"metadata":{"id":"Wrf9bHc3c1ux"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Modify last layer of pre trained model**"],"metadata":{"id":"TO_einhfj6jW"}},{"cell_type":"markdown","source":["This is example of resnet model for computer vision task. I will modify last layer of this whole network. Last layer of resnet50 contain input=2048 and output=1000."],"metadata":{"id":"8bcnXD3uj-zm"}},{"cell_type":"code","source":["from torchvision import models\n","resnet_model = models.resnet50(pretrained=True)\n","resnet_model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2PHMzkToi47H","executionInfo":{"status":"ok","timestamp":1681723772436,"user_tz":-420,"elapsed":2932,"user":{"displayName":"Wahyu Dwi Nugraha","userId":"00900073614664055541"}},"outputId":"a486403c-92e6-4cab-f115-b82bfd185a6b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","100%|██████████| 97.8M/97.8M [00:01<00:00, 72.6MB/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (4): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (5): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",")"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["From (fc) resnet above, we can see there are in_features=2048 and out_features=1000. Out_features = 1000 because resnet is pre trained on imagenet dataset which contain 1000 classes."],"metadata":{"id":"m5GUzyy9kOLI"}},{"cell_type":"code","source":["# Modify last layer and change out_features from 1000 to 4 classes\n","import torch.nn as nn\n","resnet_model.fc = nn.Linear(2048, 4, bias=True)"],"metadata":{"id":"e7jMlMUoi_x0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["resnet_model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xdqTKKdsjVD0","executionInfo":{"status":"ok","timestamp":1681723972475,"user_tz":-420,"elapsed":3,"user":{"displayName":"Wahyu Dwi Nugraha","userId":"00900073614664055541"}},"outputId":"3c09e40c-fc9d-45bf-dd92-51cc95a7ec76"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (4): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (5): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=2048, out_features=4, bias=True)\n",")"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["From network report above, in (fc) we can see out_features has been changed to 4."],"metadata":{"id":"rWpbquu7kqks"}},{"cell_type":"markdown","source":["**Pre trained model with torch hub**"],"metadata":{"id":"DwY60vvNlWxG"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn"],"metadata":{"id":"9_7xwLY7GWmf","executionInfo":{"status":"ok","timestamp":1684564751476,"user_tz":-420,"elapsed":3998,"user":{"displayName":"Wahyu Dwi Nugraha","userId":"00900073614664055541"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["model_vgg19 = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19', pretrained=True)\n","model_vgg19"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2XpVLFmZj1Qb","executionInfo":{"status":"ok","timestamp":1684564763191,"user_tz":-420,"elapsed":11720,"user":{"displayName":"Wahyu Dwi Nugraha","userId":"00900073614664055541"}},"outputId":"bd1cbc6b-50cc-46a4-891c-37c1e3789305"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n","Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n","100%|██████████| 548M/548M [00:06<00:00, 82.6MB/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["VGG(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (6): ReLU(inplace=True)\n","    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): ReLU(inplace=True)\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): ReLU(inplace=True)\n","    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): ReLU(inplace=True)\n","    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (17): ReLU(inplace=True)\n","    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (20): ReLU(inplace=True)\n","    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (22): ReLU(inplace=True)\n","    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (24): ReLU(inplace=True)\n","    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (26): ReLU(inplace=True)\n","    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (29): ReLU(inplace=True)\n","    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (31): ReLU(inplace=True)\n","    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (33): ReLU(inplace=True)\n","    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (35): ReLU(inplace=True)\n","    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n","  (classifier): Sequential(\n","    (0): Linear(in_features=25088, out_features=4096, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=4096, out_features=4096, bias=True)\n","    (4): ReLU(inplace=True)\n","    (5): Dropout(p=0.5, inplace=False)\n","    (6): Linear(in_features=4096, out_features=1000, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["# select spesific layer in vgg features\n","model_vgg19.features[0] # select layer 0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jYq-KzvZlHL0","executionInfo":{"status":"ok","timestamp":1682521576298,"user_tz":-420,"elapsed":2,"user":{"displayName":"Wahyu Dwi Nugraha","userId":"00900073614664055541"}},"outputId":"723728ee-280f-42e6-a0c4-3a21f293db4a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# Change last layer hyperparameter value\n","model_vgg19.classifier[6] = nn.Linear(in_features=4096, out_features=6, bias=True)"],"metadata":{"id":"SE3ZkqvqlRA0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_vgg19"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CBB5WnPwlvG2","executionInfo":{"status":"ok","timestamp":1682521605493,"user_tz":-420,"elapsed":308,"user":{"displayName":"Wahyu Dwi Nugraha","userId":"00900073614664055541"}},"outputId":"e52ace52-ccf8-47d8-a4c4-ac91855065c6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["VGG(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (6): ReLU(inplace=True)\n","    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): ReLU(inplace=True)\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): ReLU(inplace=True)\n","    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): ReLU(inplace=True)\n","    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (17): ReLU(inplace=True)\n","    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (20): ReLU(inplace=True)\n","    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (22): ReLU(inplace=True)\n","    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (24): ReLU(inplace=True)\n","    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (26): ReLU(inplace=True)\n","    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (29): ReLU(inplace=True)\n","    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (31): ReLU(inplace=True)\n","    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (33): ReLU(inplace=True)\n","    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (35): ReLU(inplace=True)\n","    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n","  (classifier): Sequential(\n","    (0): Linear(in_features=25088, out_features=4096, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=4096, out_features=4096, bias=True)\n","    (4): ReLU(inplace=True)\n","    (5): Dropout(p=0.5, inplace=False)\n","    (6): Linear(in_features=4096, out_features=6, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["Last layer output features successfully changed to 6."],"metadata":{"id":"zM42KG8Hm2ge"}},{"cell_type":"markdown","source":["**Freeze layer parameters**"],"metadata":{"id":"ZyMoMwfaGKWH"}},{"cell_type":"code","source":["for name,module in model_vgg19.named_children():\n","  if name != 'classifier':\n","    for a in module.parameters():\n","      a.requires_grad = False\n","\n","  else:\n","    for b in range(len(model_vgg19.classifier)):\n","      if b != 6:\n","        for c in model_vgg19.classifier[b].parameters():\n","          c.requires_grad = False"],"metadata":{"id":"gfn1ZlBtGO7h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchsummary import summary\n","\n","model_vgg19 = model_vgg19.to(device='cpu')\n","\n","summary(model_vgg19, (3,224,224))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9eJhzDbDI97u","executionInfo":{"status":"ok","timestamp":1682522282846,"user_tz":-420,"elapsed":932,"user":{"displayName":"Wahyu Dwi Nugraha","userId":"00900073614664055541"}},"outputId":"8ba369ea-22e5-4acf-e432-966019fe7878"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 224, 224]           1,792\n","              ReLU-2         [-1, 64, 224, 224]               0\n","            Conv2d-3         [-1, 64, 224, 224]          36,928\n","              ReLU-4         [-1, 64, 224, 224]               0\n","         MaxPool2d-5         [-1, 64, 112, 112]               0\n","            Conv2d-6        [-1, 128, 112, 112]          73,856\n","              ReLU-7        [-1, 128, 112, 112]               0\n","            Conv2d-8        [-1, 128, 112, 112]         147,584\n","              ReLU-9        [-1, 128, 112, 112]               0\n","        MaxPool2d-10          [-1, 128, 56, 56]               0\n","           Conv2d-11          [-1, 256, 56, 56]         295,168\n","             ReLU-12          [-1, 256, 56, 56]               0\n","           Conv2d-13          [-1, 256, 56, 56]         590,080\n","             ReLU-14          [-1, 256, 56, 56]               0\n","           Conv2d-15          [-1, 256, 56, 56]         590,080\n","             ReLU-16          [-1, 256, 56, 56]               0\n","           Conv2d-17          [-1, 256, 56, 56]         590,080\n","             ReLU-18          [-1, 256, 56, 56]               0\n","        MaxPool2d-19          [-1, 256, 28, 28]               0\n","           Conv2d-20          [-1, 512, 28, 28]       1,180,160\n","             ReLU-21          [-1, 512, 28, 28]               0\n","           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n","             ReLU-23          [-1, 512, 28, 28]               0\n","           Conv2d-24          [-1, 512, 28, 28]       2,359,808\n","             ReLU-25          [-1, 512, 28, 28]               0\n","           Conv2d-26          [-1, 512, 28, 28]       2,359,808\n","             ReLU-27          [-1, 512, 28, 28]               0\n","        MaxPool2d-28          [-1, 512, 14, 14]               0\n","           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n","             ReLU-30          [-1, 512, 14, 14]               0\n","           Conv2d-31          [-1, 512, 14, 14]       2,359,808\n","             ReLU-32          [-1, 512, 14, 14]               0\n","           Conv2d-33          [-1, 512, 14, 14]       2,359,808\n","             ReLU-34          [-1, 512, 14, 14]               0\n","           Conv2d-35          [-1, 512, 14, 14]       2,359,808\n","             ReLU-36          [-1, 512, 14, 14]               0\n","        MaxPool2d-37            [-1, 512, 7, 7]               0\n","AdaptiveAvgPool2d-38            [-1, 512, 7, 7]               0\n","           Linear-39                 [-1, 4096]     102,764,544\n","             ReLU-40                 [-1, 4096]               0\n","          Dropout-41                 [-1, 4096]               0\n","           Linear-42                 [-1, 4096]      16,781,312\n","             ReLU-43                 [-1, 4096]               0\n","          Dropout-44                 [-1, 4096]               0\n","           Linear-45                    [-1, 6]          24,582\n","================================================================\n","Total params: 139,594,822\n","Trainable params: 24,582\n","Non-trainable params: 139,570,240\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 238.68\n","Params size (MB): 532.51\n","Estimated Total Size (MB): 771.77\n","----------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["from model summary above, we can see that trainable parameters only 24,582 (for last layer only)."],"metadata":{"id":"PnTLAk8DJNHx"}},{"cell_type":"markdown","source":["## **Save and load model**"],"metadata":{"id":"fa0Jqus28lbz"}},{"cell_type":"code","source":["#save model\n","torch.save(model.state_dict(), 'model.pth')\n","\n","#load model\n","loaded_model = Model(n_input_features=6)\n","loaded_model.load_state_dict(torch.load('model.pth'))"],"metadata":{"id":"Kp_UuGk4m001"},"execution_count":null,"outputs":[]}]}